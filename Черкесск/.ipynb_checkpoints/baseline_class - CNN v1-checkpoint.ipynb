{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Gzuv2run9Yxa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5MpBxb2MjsO"
   },
   "source": [
    "## Датасет\n",
    "\n",
    "Прежде чем разбираться с моделями, нам надо в первую очередь разобраться с тем, как грузить датасет. Давайте напишем класс в торче для этого."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JtcHBYJ9oxVx"
   },
   "outputs": [],
   "source": [
    "# Объявляем переменные файлов и папки\n",
    "DIR_TRAIN = \"../train/train/\"\n",
    "DIR_TEST = \"../test/test/\"\n",
    "\n",
    "PATH_TRAIN = \"../train/train.csv\"\n",
    "PATH_TEST = \"../test/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Открываем маппинг изображений и классов\n",
    "train_map = pd.read_csv(PATH_TRAIN)\n",
    "test_map = pd.read_csv(PATH_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем папки для классификации\n",
    "for label in train_map['class'].value_counts().index:\n",
    "    os.makedirs(DIR_TRAIN + str(int(label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем папки для классификации\n",
    "for label in train_map['class'].value_counts().index:\n",
    "    os.makedirs(DIR_TEST + str(int(label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Перемещаем файлы по папкам классов\n",
    "def move_file(files_dir, file, category):\n",
    "    shutil.move(files_dir + file, files_dir + f\"{int(category)}/\" + file)\n",
    "    \n",
    "train_map.apply(lambda x: move_file(DIR_TRAIN, x['ID_img'], x['class']), axis = 1)\n",
    "test_map.apply(lambda x: move_file(DIR_TEST, x['ID_img'], x['class']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4990 files belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "ds_train = image_dataset_from_directory(\n",
    "    DIR_TRAIN,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    image_size=[IMG_SIZE, IMG_SIZE],\n",
    "    batch_size=16,\n",
    "    interpolation=\"bilinear\",\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2138 files belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "ds_valid = image_dataset_from_directory(\n",
    "    DIR_TEST,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    image_size=[IMG_SIZE, IMG_SIZE],\n",
    "    batch_size=16,\n",
    "    interpolation=\"bilinear\",\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    \n",
    "#     layers.Rescaling(1./255),\n",
    "    # Block One\n",
    "    layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same',\n",
    "                  input_shape=[IMG_SIZE, IMG_SIZE, 3]),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Block Two\n",
    "    layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Block Three\n",
    "    layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    # Head\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(6, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(8, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(epsilon=0.01),\n",
    "    loss = 'SparseCategoricalCrossentropy',\n",
    "    metrics = ['SparseCategoricalAccuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_valid,\n",
    "    epochs=50,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
